{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import imageio.v3 as iio # Library to handle image/video file saving\n",
        "import time\n",
        "import os\n",
        "\n",
        "# --- Part 1: Training the Q-Learning Agent (Same as before) ---\n",
        "\n",
        "# 1. Environment and Q-Table Setup\n",
        "# Use the default render_mode for training (None), as we don't need frames yet.\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "Q_matrix = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# 2. Hyperparameters\n",
        "LEARNING_RATE = 0.4\n",
        "DISCOUNT_FACTOR = 0.6\n",
        "EXPLORATION_RATE = 0.7\n",
        "NUM_EPISODES = 10000\n",
        "\n",
        "# 3. Training Loop\n",
        "print(\"--- Starting Training ---\")\n",
        "for episode in range(NUM_EPISODES):\n",
        "    current_state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Epsilon-Greedy Strategy:\n",
        "        if np.random.uniform(0, 1) < EXPLORATION_RATE:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(Q_matrix[current_state]) # Exploit\n",
        "\n",
        "        # Take action and observe results\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Q-Learning Update Formula\n",
        "        old_value = Q_matrix[current_state, action]\n",
        "        next_max = np.max(Q_matrix[next_state])\n",
        "        new_value = (1 - LEARNING_RATE) * old_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max)\n",
        "        Q_matrix[current_state, action] = new_value\n",
        "\n",
        "        current_state = next_state\n",
        "\n",
        "    EXPLORATION_RATE = max(0.01, EXPLORATION_RATE * 0.9995)\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode: {episode}\")\n",
        "\n",
        "print(\"--- Training Complete ---\")\n",
        "env.close()\n",
        "\n",
        "\n",
        "# --- Part 2: Visualization and GIF Generation ---\n",
        "\n",
        "# 1. Setup for Rendering\n",
        "# Create a new environment instance with 'rgb_array' to get the pixel data\n",
        "env_render = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "state, info = env_render.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "steps = 0\n",
        "FRAME_COLLECTION = []\n",
        "\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "print(\"--- Testing and Generating GIF ---\")\n",
        "\n",
        "# Capture the initial state frame\n",
        "FRAME_COLLECTION.append(env_render.render())\n",
        "\n",
        "while not done:\n",
        "    # 2. Exploit: Use the trained Q-matrix to choose the best action\n",
        "    action = np.argmax(Q_matrix[state])\n",
        "\n",
        "    # 3. Take the action\n",
        "    state, reward, terminated, truncated, info = env_render.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "    # 4. Capture the frame after the action is taken\n",
        "    FRAME_COLLECTION.append(env_render.render())\n",
        "\n",
        "    # Stop if the path is excessively long (safety break)\n",
        "    if steps > 100:\n",
        "        print(\"Stopping visualization after 100 steps to prevent infinite loop.\")\n",
        "        break\n",
        "\n",
        "# 5. Save the collected frames as a GIF\n",
        "gif_filename = \"taxi_driver_path.gif\"\n",
        "try:\n",
        "    # duration is the time (in seconds) between each frame\n",
        "    iio.imwrite(gif_filename, FRAME_COLLECTION, duration=0.5, loop=0)\n",
        "    print(f\"\\nSuccessfully created visualization: **{gif_filename}**\")\n",
        "    print(f\"Episode finished in {steps} steps with a total reward of: {total_reward}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving GIF: {e}\")\n",
        "    print(\"Ensure you have 'imageio' installed (`pip install imageio`).\")\n",
        "\n",
        "\n",
        "# Clean up the environment\n",
        "env_render.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwr269mZBUR3",
        "outputId": "c1adfa8e-dd60-4009-b79d-3ea7c16084e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Training ---\n",
            "Episode: 0\n",
            "Episode: 1000\n",
            "Episode: 2000\n",
            "Episode: 3000\n",
            "Episode: 4000\n",
            "Episode: 5000\n",
            "Episode: 6000\n",
            "Episode: 7000\n",
            "Episode: 8000\n",
            "Episode: 9000\n",
            "--- Training Complete ---\n",
            "\n",
            "========================================\n",
            "\n",
            "--- Testing and Generating GIF ---\n",
            "\n",
            "Successfully created visualization: **taxi_driver_path.gif**\n",
            "Episode finished in 15 steps with a total reward of: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgn9LgKSAES6",
        "outputId": "f57c4db4-adf9-40f9-957c-9538c80c0c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Training ---\n",
            "Episode: 0\n",
            "Episode: 1000\n",
            "Episode: 2000\n",
            "Episode: 3000\n",
            "Episode: 4000\n",
            "Episode: 5000\n",
            "Episode: 6000\n",
            "Episode: 7000\n",
            "Episode: 8000\n",
            "Episode: 9000\n",
            "--- Training Complete ---\n",
            "Q-Table head (State 0-4):\n",
            "[[  0.           0.           0.           0.           0.\n",
            "    0.        ]\n",
            " [ -2.41837019  -2.36395111  -2.41837066  -2.3639511   -2.27325184\n",
            "  -11.363905  ]\n",
            " [ -1.87014402  -1.45030817  -1.87014405  -1.45024209  -0.7504\n",
            "  -10.45024006]\n",
            " [ -2.36395069  -2.27325179  -2.36395106  -2.27324301  -2.1220864\n",
            "  -11.27325083]\n",
            " [ -2.49618753  -2.49661449  -2.49618836  -2.49649829 -11.48632629\n",
            "  -11.49325828]]\n",
            "\n",
            "========================================\n",
            "\n",
            "--- Testing and Visualizing Trained Agent ---\n",
            "\n",
            "Episode finished in 11 steps with a total reward of: 10\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython.display import clear_output # For use in Jupyter/Colab notebooks\n",
        "\n",
        "# --- Part 1: Training the Q-Learning Agent ---\n",
        "\n",
        "# 1. Environment and Q-Table Setup\n",
        "# Use render_mode=\"ansi\" for console output during training if desired, but \"rgb_array\"\n",
        "# or \"human\" is needed for the final visualization (Part 2).\n",
        "env = gym.make(\"Taxi-v3\")\n",
        "Q_matrix = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# 2. Hyperparameters\n",
        "LEARNING_RATE = 0.4    # alpha\n",
        "DISCOUNT_FACTOR = 0.6  # gamma\n",
        "EXPLORATION_RATE = 0.7 # epsilon (starting value)\n",
        "NUM_EPISODES = 10000\n",
        "\n",
        "# 3. Training Loop\n",
        "print(\"--- Starting Training ---\")\n",
        "for episode in range(NUM_EPISODES):\n",
        "    current_state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Epsilon-Greedy Strategy:\n",
        "        if np.random.uniform(0, 1) < EXPLORATION_RATE:\n",
        "            action = env.action_space.sample()  # Explore: Choose random action\n",
        "        else:\n",
        "            action = np.argmax(Q_matrix[current_state]) # Exploit: Choose max Q-value\n",
        "\n",
        "        # Take action and observe results\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Q-Learning Update Formula\n",
        "        # Q(s,a) = (1-lr) * Q(s,a) + lr * (Reward + gamma * max(Q(s', a')))\n",
        "        old_value = Q_matrix[current_state, action]\n",
        "        next_max = np.max(Q_matrix[next_state])\n",
        "        new_value = (1 - LEARNING_RATE) * old_value + LEARNING_RATE * (reward + DISCOUNT_FACTOR * next_max)\n",
        "        Q_matrix[current_state, action] = new_value\n",
        "\n",
        "        current_state = next_state\n",
        "\n",
        "    # Simple decay for the exploration rate over time\n",
        "    EXPLORATION_RATE = max(0.01, EXPLORATION_RATE * 0.9995)\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode: {episode}\")\n",
        "\n",
        "print(\"--- Training Complete ---\")\n",
        "print(\"Q-Table head (State 0-4):\")\n",
        "print(Q_matrix[:5])\n",
        "print(\"\\n\" + \"=\"*40 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- Part 2: Visualization of the Trained Agent ---\n",
        "\n",
        "# Re-create the environment with 'human' rendering mode for real-time visualization\n",
        "# Note: For Jupyter/Colab, you might need 'rgb_array' and imageio to save a GIF.\n",
        "env_render = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
        "# Reset the environment for the demonstration\n",
        "state, info = env_render.reset()\n",
        "done = False\n",
        "total_reward = 0\n",
        "steps = 0\n",
        "\n",
        "print(\"--- Testing and Visualizing Trained Agent ---\")\n",
        "\n",
        "while not done:\n",
        "    # 1. Exploit: Use the trained Q-matrix to choose the best action\n",
        "    action = np.argmax(Q_matrix[state])\n",
        "\n",
        "    # 2. Take the action\n",
        "    state, reward, terminated, truncated, info = env_render.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    total_reward += reward\n",
        "    steps += 1\n",
        "\n",
        "    # 3. Render the environment\n",
        "    # The 'human' render mode displays the window automatically.\n",
        "\n",
        "    # Optional: Slow down the visualization\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    # Optional: Clear output for clean console display (useful for 'ansi' render_mode)\n",
        "    # clear_output(wait=True)\n",
        "\n",
        "# The visualization output will look like a 5x5 grid in a separate window,\n",
        "# with the blue/yellow taxi ('T') moving to the passenger ('R', 'G', 'Y', 'B'),\n",
        "# picking them up (the taxi turns green with the passenger inside),\n",
        "# and dropping them off at the destination.\n",
        "#\n",
        "\n",
        "print(f\"\\nEpisode finished in {steps} steps with a total reward of: {total_reward}\")\n",
        "\n",
        "# Clean up the environment\n",
        "env.close()\n",
        "env_render.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_gGVeYZkAKL2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for e in range(episodes):\n",
        "    current_state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # 1. Choose action: Explore (random) or Exploit (max Q-value)\n",
        "        if np.random.uniform(0, 1) < exploration_factor:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            action = np.argmax(Q_matrix[current_state]) # Exploit\n",
        "\n",
        "        # 2. Take action and observe results\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # 3. Update the Q-Table using the Q-Learning formula\n",
        "        # Q(s,a) = (1-lr) * Q(s,a) + lr * (Reward + discount_factor * max(Q(s', a')))\n",
        "        Q_matrix[current_state, action] = (1.0 - lr) * Q_matrix[current_state, action] + \\\n",
        "                                           lr * (reward + discount_factor * np.max(Q_matrix[next_state]))\n",
        "\n",
        "        current_state = next_state"
      ],
      "metadata": {
        "id": "La6WBnSwAKJ0"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}